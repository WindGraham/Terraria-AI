#!/usr/bin/env python3
"""
æ™ºèƒ½ Wiki æ•°æ®æ¸…æ´—è„šæœ¬
åŠŸèƒ½ï¼š
1. å»é™¤HTMLæ ‡ç­¾ä½†ä¿ç•™æ–‡æœ¬ç»“æ„
2. å»é™¤æ— ç”¨éƒ¨åˆ†ï¼ˆç‰ˆæœ¬è­¦å‘Šã€å†å²ã€å‚è€ƒã€å›¾åº“ã€å¯¼èˆªç­‰ï¼‰
3. ä¿ç•™æœ‰ç”¨éƒ¨åˆ†ï¼ˆä¿¡æ¯æ¡†ã€å¬å”¤ã€è¡Œä¸ºã€å°è´´å£«ã€æ‰è½ã€èŠ±çµ®ã€æˆå°±ï¼‰
4. åªä¿ç•™ä¸­æ–‡å†…å®¹ï¼ˆä¸­æ–‡æ¯”ä¾‹>85%ï¼‰
5. è¿‡æ»¤åˆ—è¡¨é¡µå’Œç³»ç»Ÿé¡µ
"""

import json
import os
import re
import html
from pathlib import Path
from html.parser import HTMLParser

# é…ç½®
SOURCE_DIR = Path.home() / "Projects/TerrariaWiki/terraria_wiki/wiki_full_data"
TARGET_DIR = Path.home() / "Projects/TerrariaWiki/terraria_wiki/wiki_cleaned"
BATCH_SIZE = 500

# éœ€è¦è·³è¿‡çš„é¡µé¢æ¨¡å¼
SKIP_PATTERNS = [
    r'^List ',
    r'^åˆ—è¡¨',
    r'^Item ',
    r'^Items ',
    r'^Category:',
    r'^File:',
    r'^Template:',
    r'^User:',
    r'^Talk:',
    r'^Old:',
    r'^Legacy:',
    r'^Guide:',  # è‹±æ–‡æ”»ç•¥é¡µ
    r'^[0-9]+\.[0-9]+',  # ç‰ˆæœ¬å·é¡µé¢
    r'^æ›´æ–°æ—¥å¿—',
    r'^ç‰ˆæœ¬å†å²',
    r'^Enchanting',
    r'^Seed ',
    r'^Achievements ',  # æˆå°±åˆ—è¡¨
    r'^Buffs',
    r'^Debuffs',
    r'^Config\.json',
]

SKIP_REGEX = [re.compile(p, re.IGNORECASE) for p in SKIP_PATTERNS]


class HTMLStripper(HTMLParser):
    """HTMLæ ‡ç­¾å‰¥ç¦»å™¨ï¼Œä¿ç•™æ–‡æœ¬ç»“æ„"""
    
    def __init__(self):
        super().__init__()
        self.reset()
        self.fed = []
        self.in_skip_tag = 0
        self.skip_tags = {'script', 'style', 'noscript'}
        
    def handle_starttag(self, tag, attrs):
        if tag in self.skip_tags:
            self.in_skip_tag += 1
            
    def handle_endtag(self, tag):
        if tag in self.skip_tags and self.in_skip_tag > 0:
            self.in_skip_tag -= 1
            
    def handle_data(self, d):
        if self.in_skip_tag == 0:
            self.fed.append(d)
            
    def get_data(self):
        return ''.join(self.fed)


def strip_html_tags(html_text):
    """å»é™¤HTMLæ ‡ç­¾ï¼Œä¿ç•™çº¯æ–‡æœ¬"""
    if not html_text:
        return ""
    
    # ä½¿ç”¨HTMLParserå‰¥ç¦»æ ‡ç­¾
    stripper = HTMLStripper()
    try:
        stripper.feed(html_text)
        text = stripper.get_data()
    except:
        # å¦‚æœè§£æå¤±è´¥ï¼Œä½¿ç”¨æ­£åˆ™å›é€€
        text = re.sub(r'<script[^>]*>.*?</script>', '', html_text, flags=re.DOTALL)
        text = re.sub(r'<style[^>]*>.*?</style>', '', text, flags=re.DOTALL)
        text = re.sub(r'<[^>]+>', '\n', text)
    
    # è§£ç HTMLå®ä½“
    text = html.unescape(text)
    
    # æ¸…ç†å¤šä½™ç©ºç™½
    text = re.sub(r'\n\s*\n+', '\n\n', text)
    text = re.sub(r'[ \t]+', ' ', text)
    
    return text.strip()


def remove_useless_sections(text):
    """å»é™¤æ— ç”¨ç« èŠ‚"""
    
    # å®šä¹‰è¦å»é™¤çš„ç« èŠ‚æ ‡é¢˜æ¨¡å¼
    remove_headers = [
        r'å†å²\s*$',
        r'å‚è€ƒ\s*$',
        r'è„šæ³¨\s*$',
        r'å›¾åº“\s*$',
        r'å¦è§\s*$',
        r'ç›¸å…³é“¾æ¥\s*$',
        r'å¯¼èˆª\s*$',
    ]
    
    lines = text.split('\n')
    result = []
    skip_until_next_header = False
    
    for line in lines:
        line_stripped = line.strip()
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯ç« èŠ‚æ ‡é¢˜ï¼ˆé€šå¸¸æ˜¯ç®€çŸ­è¡Œï¼‰
        is_header = len(line_stripped) < 30 and not skip_until_next_header
        
        if is_header:
            for pattern in remove_headers:
                if re.search(pattern, line_stripped):
                    skip_until_next_header = True
                    break
        
        # å¦‚æœæ˜¯æ–°çš„ç« èŠ‚æ ‡é¢˜ï¼Œåœæ­¢è·³è¿‡
        if skip_until_next_header and line_stripped and len(line_stripped) < 30:
            is_new_section = any(keyword in line_stripped for keyword in [
                'å¬å”¤', 'è¡Œä¸º', 'æ”»å‡»', 'é˜²å¾¡', 'æ‰è½', 'å°è´´å£«', 'å¤‡æ³¨', 
                'èŠ±çµ®', 'æˆå°±', 'ä¿¡æ¯', 'å±æ€§', 'ä¼¤å®³', 'ç”Ÿå‘½'
            ])
            if is_new_section:
                skip_until_next_header = False
        
        if not skip_until_next_header:
            result.append(line)
    
    return '\n'.join(result)


def clean_version_warnings(text):
    """å»é™¤ç‰ˆæœ¬è­¦å‘Šå’Œå¹³å°ä¿¡æ¯"""
    
    # å»é™¤å¸¸è§çš„ç‰ˆæœ¬è­¦å‘Šæ–‡æœ¬
    patterns = [
        r'è¯¥é¡µé¢ä¸º.*?ä¸».*?é¡µé¢ï¼Œå…¶ä¿¡æ¯é€‚ç”¨äº.*?ç”µè„‘ç‰ˆ.*?ä¸»æœºç‰ˆ.*?ç§»åŠ¨ç‰ˆ.*?ç‰ˆæœ¬çš„ã€Šæ³°æ‹‰ç‘äºšã€‹ã€‚.*?(?=\n|$)',
        r'å¯¹äºå‰ä»£ä¸»æœºç‰ˆå’Œä»»å¤©å ‚3DSç‰ˆä¸­çš„ä¿¡æ¯å·®å¼‚ï¼Œè§.*?æ—§ç‰ˆ:.*?ã€‚',
        r'ç”µè„‘ç‰ˆç‰ˆæœ¬å†å²',
        r'ä¸»æœºç‰ˆç‰ˆæœ¬å†å²', 
        r'ç§»åŠ¨ç‰ˆç‰ˆæœ¬å†å²',
        r'å‰ä»£ä¸»æœºç‰ˆç‰ˆæœ¬å†å²',
        r'ä»»å¤©å ‚3DSç‰ˆç‰ˆæœ¬å†å²',
        r'ç”µè„‘ç‰ˆã€ä¸»æœºç‰ˆã€å’Œç§»åŠ¨ç‰ˆ',
        r'ç”µè„‘ç‰ˆã€ä¸»æœºç‰ˆã€å‰ä»£ä¸»æœºç‰ˆã€å’Œç§»åŠ¨ç‰ˆ',
        r'\(ç”µè„‘ç‰ˆã€ä¸»æœºç‰ˆã€å’Œç§»åŠ¨ç‰ˆ\)',
        r'\(ç”µè„‘ç‰ˆã€ä¸»æœºç‰ˆã€å‰ä»£ä¸»æœºç‰ˆã€å’Œç§»åŠ¨ç‰ˆ\)',
        r'\(å‰ä»£ä¸»æœºç‰ˆã€å’Œ3DSç‰ˆ\)',
        r'\(3DSç‰ˆ\)',
        r'&#\d+;',  # HTMLå®ä½“ç¼–ç 
        # å»é™¤ç‰ˆæœ¬æ›´æ–°è®°å½•ï¼ˆå¦‚"ç”µè„‘ç‰ˆ 1.3.0.1ï¼šå¼•å…¥"ï¼‰
        r'ç”µè„‘ç‰ˆ\s+\d+\.\d+(\.\d+)*\s*[:ï¼š].*?(?=\n|$)',
        r'ä¸»æœºç‰ˆ\s+\d+\.\d+(\.\d+)*\s*[:ï¼š].*?(?=\n|$)',
        r'ç§»åŠ¨ç‰ˆ\s+\d+\.\d+(\.\d+)*\s*[:ï¼š].*?(?=\n|$)',
        r'Switchç‰ˆ\s+\d+\.\d+(\.\d+)*\s*[:ï¼š].*?(?=\n|$)',
        r'\d+\.\d+(\.\d+)*\s*[:ï¼š]\s*(å¼•å…¥|ä¿®æ”¹|ä¿®å¤).*?(?=\n|$)',
        # å»é™¤å¹³å°æ ‡ç­¾
        r'\s*ä¸»æœºç‰ˆ\s*',
        r'\s*ç§»åŠ¨ç‰ˆ\s*',
        r'\s*Switchç‰ˆ\s*',
        r'\s*ä»»å¤©å ‚Switchç‰ˆ\s*',
        r'\s*3DSç‰ˆ\s*',
        r'\s*å‰ä»£ä¸»æœºç‰ˆ\s*',
    ]
    
    for pattern in patterns:
        text = re.sub(pattern, '', text, flags=re.DOTALL)
    
    return text


def remove_category_tags(text):
    """å»é™¤åˆ†ç±»æ ‡ç­¾ç›¸å…³æ–‡æœ¬"""
    
    patterns = [
        r'ç”µè„‘ç‰ˆ_\d+\.\d+(_\d+)*_ä¸­.*?çš„å®ä½“',
        r'ä¸»æœºç‰ˆ_\d+\.\d+(_\d+)*_ä¸­.*?çš„å®ä½“',
        r'ç§»åŠ¨ç‰ˆ_\d+\.\d+(_\d+)*_ä¸­.*?çš„å®ä½“',
        r'Switchç‰ˆ_\d+\.\d+(_\d+)*_ä¸­.*?çš„å®ä½“',
        r'\d+_æ­£å¼ç‰ˆä¸­å¼•å…¥çš„å®ä½“',
        r'Pages_with_navboxes',
        r'Pages_setting_LuaCache_keys',
        r'ä½¿ç”¨DynamicPageList',
        r'å«æœ‰éæ•°å­—formatnumå‚æ•°çš„é¡µé¢',
        r'é¡µé¢ä¸Šæœ‰ä¿¡æ¯åŸºäºçš„æ˜¯è¿‡æ—¶ç‰ˆæœ¬çš„æ³°æ‹‰ç‘äºšæºä»£ç ',
        r'æˆå°±ç›¸å…³å…ƒç´ ',
        r'é¥¥è’è”åŠ¨å†…å®¹',
        r'ç¨€æœ‰åº¦ä¸º.*?çš„ç‰©å“',
        r'ç‹¬æœ‰å†…å®¹',
    ]
    
    for pattern in patterns:
        text = re.sub(pattern, '', text)
    
    return text


def clean_categories(categories):
    """æ¸…æ´—categoriesåˆ—è¡¨ï¼Œå»é™¤ç‰ˆæœ¬ç›¸å…³çš„åˆ†ç±»"""
    cleaned = []
    skip_patterns = [
        r'ç”µè„‘ç‰ˆ_\d+\.\d+',
        r'ä¸»æœºç‰ˆ_\d+\.\d+',
        r'ç§»åŠ¨ç‰ˆ_\d+\.\d+',
        r'Switchç‰ˆ_\d+\.\d+',
        r'\d+_æ­£å¼ç‰ˆ',
        r'Pages_',
        r'ä½¿ç”¨DynamicPageList',
        r'å«æœ‰éæ•°å­—formatnum',
        r'é¡µé¢ä¸Šæœ‰ä¿¡æ¯åŸºäº',
        r'æˆå°±ç›¸å…³',
        r'ç¨€æœ‰åº¦ä¸º',
        r'ç‹¬æœ‰å†…å®¹',
    ]
    
    for cat in categories:
        cat_text = cat['*'] if isinstance(cat, dict) else cat
        should_skip = False
        for pattern in skip_patterns:
            if re.search(pattern, cat_text):
                should_skip = True
                break
        if not should_skip:
            cleaned.append(cat_text)
    
    return cleaned


def calculate_chinese_ratio(text):
    """è®¡ç®—ä¸­æ–‡å­—ç¬¦æ¯”ä¾‹"""
    if not text:
        return 0
    chinese_chars = len(re.findall(r'[\u4e00-\u9fff]', text))
    all_letters = len(re.findall(r'[a-zA-Z\u4e00-\u9fff]', text))
    return chinese_chars / all_letters if all_letters > 0 else 0


def should_skip_file(filename, title):
    """åˆ¤æ–­æ˜¯å¦åº”è¯¥è·³è¿‡æ­¤æ–‡ä»¶"""
    
    # æ£€æŸ¥æ–‡ä»¶åæ¨¡å¼
    for pattern in SKIP_REGEX:
        if pattern.match(filename.replace('.json', '')):
            return True
    
    # æ£€æŸ¥æ ‡é¢˜æ¨¡å¼
    for pattern in SKIP_REGEX:
        if pattern.match(title):
            return True
    
    return False


def clean_single_file(filepath):
    """æ¸…æ´—å•ä¸ªæ–‡ä»¶"""
    try:
        with open(filepath, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        title = data.get('title', '')
        filename = filepath.name
        
        # æ£€æŸ¥æ˜¯å¦åº”è¯¥è·³è¿‡
        if should_skip_file(filename, title):
            return None, "skipped"
        
        # æå–HTMLå†…å®¹
        text_data = data.get('text', {})
        if isinstance(text_data, dict):
            html_content = text_data.get('*', '')
        else:
            html_content = str(text_data)
        
        # 1. å»é™¤HTMLæ ‡ç­¾
        clean_text = strip_html_tags(html_content)
        
        # 2. å»é™¤ç‰ˆæœ¬è­¦å‘Š
        clean_text = clean_version_warnings(clean_text)
        
        # 3. å»é™¤åˆ†ç±»æ ‡ç­¾
        clean_text = remove_category_tags(clean_text)
        
        # 4. å»é™¤æ— ç”¨ç« èŠ‚
        clean_text = remove_useless_sections(clean_text)
        
        # 5. æœ€ç»ˆæ¸…ç†
        clean_text = re.sub(r'\n\s*\n+', '\n\n', clean_text)
        clean_text = clean_text.strip()
        
        # 6. æ£€æŸ¥ä¸­æ–‡æ¯”ä¾‹
        chinese_ratio = calculate_chinese_ratio(clean_text)
        if chinese_ratio < 0.85:
            return None, "low_chinese"
        
        # 7. æ£€æŸ¥å†…å®¹é•¿åº¦
        if len(clean_text) < 500:
            return None, "too_short"
        
        # 8. æ¸…æ´—categories
        raw_categories = data.get('categories', [])
        cleaned_categories = clean_categories(raw_categories)
        
        # 9. æ„å»ºæ¸…æ´—åçš„æ•°æ®
        cleaned = {
            'title': title,
            'content': clean_text,
            'content_length': len(clean_text),
            'chinese_ratio': round(chinese_ratio, 3),
            'categories': cleaned_categories,
        }
        
        return cleaned, "success"
        
    except Exception as e:
        return None, f"error: {e}"


def process_batch(files, batch_num, total_batches):
    """å¤„ç†ä¸€æ‰¹æ–‡ä»¶"""
    stats = {
        'processed': 0,
        'skipped': 0,
        'low_chinese': 0,
        'too_short': 0,
        'error': 0
    }
    
    for filepath in files:
        result, status = clean_single_file(filepath)
        
        if status == "success":
            # ä¿å­˜æ¸…æ´—åçš„æ–‡ä»¶
            target_path = TARGET_DIR / filepath.name
            with open(target_path, 'w', encoding='utf-8') as f:
                json.dump(result, f, ensure_ascii=False, indent=2)
            stats['processed'] += 1
        elif status == "skipped":
            stats['skipped'] += 1
        elif status == "low_chinese":
            stats['low_chinese'] += 1
        elif status == "too_short":
            stats['too_short'] += 1
        else:
            stats['error'] += 1
    
    print(f"  æ‰¹æ¬¡ {batch_num}/{total_batches}: "
          f"ä¿ç•™ {stats['processed']}, è·³è¿‡ {stats['skipped']}, "
          f"ä¸­æ–‡ä½ {stats['low_chinese']}, å¤ªçŸ­ {stats['too_short']}, é”™è¯¯ {stats['error']}")
    
    return stats


def main():
    """ä¸»å‡½æ•°"""
    print("="*70)
    print("æ™ºèƒ½ Wiki æ•°æ®æ¸…æ´—")
    print("="*70)
    
    # åˆ›å»ºç›®æ ‡ç›®å½•
    TARGET_DIR.mkdir(parents=True, exist_ok=True)
    
    # è·å–æ‰€æœ‰æ–‡ä»¶
    all_files = list(SOURCE_DIR.glob('*.json'))
    total_files = len(all_files)
    
    print(f"\næºç›®å½•: {SOURCE_DIR}")
    print(f"ç›®æ ‡ç›®å½•: {TARGET_DIR}")
    print(f"æ€»æ–‡ä»¶æ•°: {total_files}")
    
    # ç»Ÿè®¡æºç›®å½•å¤§å°
    source_size = sum(f.stat().st_size for f in all_files)
    print(f"æºæ•°æ®å¤§å°: {source_size / (1024**3):.2f} GB")
    
    print("\næ¸…æ´—è§„åˆ™:")
    print("  âŒ å»é™¤: å†å²ã€å‚è€ƒã€å›¾åº“ã€ç‰ˆæœ¬è­¦å‘Šã€å¹³å°ä¿¡æ¯ã€å¯¼èˆª")
    print("  âœ… ä¿ç•™: ä¿¡æ¯æ¡†ã€å¬å”¤ã€è¡Œä¸ºã€å°è´´å£«ã€æ‰è½ã€èŠ±çµ®ã€æˆå°±")
    print("  ğŸ“‹ æ¡ä»¶: ä¸­æ–‡æ¯”ä¾‹>85%, å†…å®¹é•¿åº¦>500å­—ç¬¦")
    print("\nå¼€å§‹æ¸…æ´—...")
    print("-"*70)
    
    # åˆ†æ‰¹å¤„ç†
    batches = [all_files[i:i+BATCH_SIZE] for i in range(0, len(all_files), BATCH_SIZE)]
    total_batches = len(batches)
    
    total_stats = {
        'processed': 0,
        'skipped': 0,
        'low_chinese': 0,
        'too_short': 0,
        'error': 0
    }
    
    for i, batch in enumerate(batches, 1):
        stats = process_batch(batch, i, total_batches)
        for key in total_stats:
            total_stats[key] += stats[key]
        
        # æ¯10æ‰¹æ˜¾ç¤ºè¿›åº¦
        if i % 10 == 0 or i == total_batches:
            progress = (i / total_batches) * 100
            print(f"\næ€»è¿›åº¦: {progress:.1f}% | "
                  f"å·²ä¿ç•™: {total_stats['processed']} | "
                  f"è·³è¿‡: {total_stats['skipped']}")
    
    print("\n" + "="*70)
    print("æ¸…æ´—å®Œæˆ!")
    print("="*70)
    
    # ç»Ÿè®¡ç»“æœ
    cleaned_files = list(TARGET_DIR.glob('*.json'))
    target_size = sum(f.stat().st_size for f in cleaned_files)
    
    print(f"\nç»Ÿè®¡ç»“æœ:")
    print(f"  åŸå§‹æ–‡ä»¶: {total_files} ä¸ª")
    print(f"  æ¸…æ´—åæ–‡ä»¶: {len(cleaned_files)} ä¸ª")
    print(f"  è·³è¿‡æ–‡ä»¶: {total_stats['skipped']} ä¸ª")
    print(f"  ä¸­æ–‡æ¯”ä¾‹è¿‡ä½: {total_stats['low_chinese']} ä¸ª")
    print(f"  å†…å®¹å¤ªçŸ­: {total_stats['too_short']} ä¸ª")
    print(f"  é”™è¯¯: {total_stats['error']} ä¸ª")
    print(f"\n  åŸå§‹å¤§å°: {source_size / (1024**3):.2f} GB")
    print(f"  æ¸…æ´—åå¤§å°: {target_size / (1024**2):.2f} MB")
    print(f"  å‹ç¼©æ¯”ä¾‹: {(1 - target_size/source_size) * 100:.1f}%")
    
    # æ˜¾ç¤ºä¸€äº›ç¤ºä¾‹
    print("\nç¤ºä¾‹æ–‡ä»¶:")
    sample_files = sorted(cleaned_files, key=lambda x: x.stat().st_size, reverse=True)[:5]
    for f in sample_files:
        size = f.stat().st_size / 1024
        print(f"  - {f.name}: {size:.1f} KB")


if __name__ == '__main__':
    main()
