# Terraria Wiki å®Œæ•´ä¸‹è½½æŒ‡å—

## ğŸ“Š æ•°æ®è§„æ¨¡

- **æ–‡ç« æ€»æ•°**: çº¦ 4,420 é¡µ
- **é¢„è®¡æ—¶é—´**: 2-4 å°æ—¶ï¼ˆå–å†³äºç½‘ç»œï¼‰
- **ç£ç›˜ç©ºé—´**: çº¦ 500MB-1GB

## ğŸš€ å¿«é€Ÿå¼€å§‹

### æ–¹æ³•ä¸€ï¼šå‰å°è¿è¡Œï¼ˆé€‚åˆæµ‹è¯•ï¼‰

```bash
cd /home/windgraham/Projects/TerrariaWiki/terraria_wiki
python crawler/download_wiki_full.py
```

### æ–¹æ³•äºŒï¼šåå°è¿è¡Œï¼ˆæ¨èï¼‰

```bash
cd /home/windgraham/Projects/TerrariaWiki/terraria_wiki

# åå°è¿è¡Œï¼Œè¾“å‡ºåˆ°æ—¥å¿—
nohup python crawler/download_wiki_full.py > download.log 2>&1 &

# æŸ¥çœ‹æ—¥å¿—
tail -f download.log

# æŸ¥çœ‹è¿›åº¦ï¼ˆJSONæ ¼å¼ï¼‰
cat download_progress.json | python -m json.tool
```

### æ–¹æ³•ä¸‰ï¼šä½¿ç”¨ Screen/Tmuxï¼ˆé˜²æ­¢SSHæ–­å¼€ï¼‰

```bash
# ä½¿ç”¨ screen
screen -S wiki_download
cd /home/windgraham/Projects/TerrariaWiki/terraria_wiki
python crawler/download_wiki_full.py
# Ctrl+A, D åˆ†ç¦»ä¼šè¯

# é‡æ–°è¿æ¥
screen -r wiki_download

# --- æˆ–ä½¿ç”¨ tmux ---
tmux new -s wiki_download
cd /home/windgraham/Projects/TerrariaWiki/terraria_wiki
python crawler/download_wiki_full.py
# Ctrl+B, D åˆ†ç¦»

# é‡æ–°è¿æ¥
tmux attach -t wiki_download
```

## ğŸ“‹ å¸¸ç”¨å‘½ä»¤

### æŸ¥çœ‹ä¸‹è½½çŠ¶æ€

```bash
# æ–¹æ³•1: æŸ¥çœ‹æ—¥å¿—
tail -f download.log

# æ–¹æ³•2: æŸ¥çœ‹è¿›åº¦æ–‡ä»¶
cat download_progress.json

# æ–¹æ³•3: ç»Ÿè®¡å·²ä¸‹è½½æ–‡ä»¶æ•°
ls wiki_full_data/ | wc -l

# æ–¹æ³•4: æŸ¥çœ‹æ•°æ®å¤§å°
du -sh wiki_full_data/
```

### æš‚åœ/æ¢å¤ä¸‹è½½

```bash
# æš‚åœï¼ˆå‘é€ Ctrl+Cï¼‰
kill -INT <è¿›ç¨‹ID>

# æˆ–è€…ç›´æ¥è¿è¡Œï¼Œä¼šè‡ªåŠ¨ç»­ä¼ 
python crawler/download_wiki_full.py
```

### é‡è¯•å¤±è´¥çš„é¡µé¢

```bash
python crawler/download_wiki_full.py --retry
```

### é‡æ–°å¼€å§‹ï¼ˆæ¸…ç©ºè¿›åº¦ï¼‰

```bash
python crawler/download_wiki_full.py --reset
```

## ğŸ“ è¾“å‡ºç»“æ„

ä¸‹è½½å®Œæˆåï¼Œ`wiki_full_data/` ç›®å½•å°†åŒ…å«ï¼š

```
wiki_full_data/
â”œâ”€â”€ æ³°æ‹‰ç‘äºš.json
â”œâ”€â”€ å…‹è‹é²ä¹‹çœ¼.json
â”œâ”€â”€ å‘å¯¼.json
â”œâ”€â”€ å•†äºº.json
â”œâ”€â”€ å‰‘.json
â”œâ”€â”€ ç›”ç”².json
â”œâ”€â”€ ... (4420+ ä¸ªæ–‡ä»¶)
â””â”€â”€ ...
```

æ¯ä¸ªæ–‡ä»¶æ ¼å¼ï¼š
```json
{
  "title": "é¡µé¢æ ‡é¢˜",
  "pageid": 12345,
  "text": { "*": "HTMLå†…å®¹..." },
  "wikitext": { "*": "Wikiæºç ..." },
  "links": [...],
  "categories": [...]
}
```

## âš™ï¸ é…ç½®æ–‡ä»¶

å¦‚éœ€è°ƒæ•´ä¸‹è½½å‚æ•°ï¼Œç¼–è¾‘ `download_wiki_full.py`ï¼š

```python
DELAY = 0.2          # è¯·æ±‚é—´éš”ï¼ˆç§’ï¼‰
BATCH_SIZE = 50      # æ¯Né¡µä¿å­˜ä¸€æ¬¡è¿›åº¦
```

## ğŸ” æ•…éšœæ’æŸ¥

### ä¸‹è½½é€Ÿåº¦å¤ªæ…¢
- æ£€æŸ¥ç½‘ç»œè¿æ¥
- å¯é€‚å½“å‡å° `DELAY` å€¼ï¼ˆä½†ä¸è¦å°äº0.1ï¼Œé¿å…è¢«å°ï¼‰

### ç»å¸¸å¤±è´¥
- æ£€æŸ¥ `download_progress.json` ä¸­çš„å¤±è´¥åˆ—è¡¨
- è¿è¡Œ `python crawler/download_wiki_full.py --retry` é‡è¯•

### ç£ç›˜ç©ºé—´ä¸è¶³
- æ£€æŸ¥ `df -h`
- æ¸…ç†ä¸å¿…è¦çš„æ–‡ä»¶

### è¿›ç¨‹è¢«æ€æ­»
- ä½¿ç”¨ `screen` æˆ– `tmux` è¿è¡Œ
- æˆ–ä½¿ç”¨ `nohup` åå°è¿è¡Œ

## ğŸ“ˆ é¢„è®¡æ—¶é—´å‚è€ƒ

| ç½‘ç»œç¯å¢ƒ | é¢„è®¡æ—¶é—´ |
|---------|---------|
| ä¼˜è´¨ç½‘ç»œ | 2-3 å°æ—¶ |
| ä¸€èˆ¬ç½‘ç»œ | 3-4 å°æ—¶ |
| è¾ƒæ…¢ç½‘ç»œ | 4-6 å°æ—¶ |

ï¼ˆåŸºäº 0.2ç§’å»¶è¿Ÿ Ã— 4420é¡µ â‰ˆ 15åˆ†é’Ÿ ç†è®ºå€¼ï¼Œå®é™…å› ç½‘ç»œæ³¢åŠ¨ä¼šæ›´é•¿ï¼‰

## âœ… å®Œæˆæ£€æŸ¥

ä¸‹è½½å®Œæˆåï¼Œè¿è¡Œä»¥ä¸‹å‘½ä»¤æ£€æŸ¥ï¼š

```bash
# ç»Ÿè®¡ä¸‹è½½æ•°é‡
echo "å·²ä¸‹è½½: $(ls wiki_full_data/ | wc -l) ä¸ªæ–‡ä»¶"

# æŸ¥çœ‹é¢„æœŸæ€»æ•°
cat download_progress.json | grep total_pages

# æ£€æŸ¥å¤±è´¥çš„é¡µé¢
cat download_progress.json | python -c "import sys,json; d=json.load(sys.stdin); print(f\"å¤±è´¥: {len(d['failed_titles'])} ä¸ª\")"
```

## ğŸ‰ ä¸‹è½½å®Œæˆå

æ•°æ®ä½äº `wiki_full_data/` ç›®å½•ï¼Œä½ å¯ä»¥ï¼š

1. **æ„å»ºæœç´¢ç´¢å¼•** - ç”¨äºå…¨æ–‡æ£€ç´¢
2. **å¯¼å…¥æ•°æ®åº“** - MongoDB/Elasticsearch
3. **æ„å»ºå‘é‡æ•°æ®åº“** - ç”¨äºAIè¯­ä¹‰æœç´¢
4. **ç”Ÿæˆé™æ€ç½‘ç«™** - è½¬æ¢ä¸ºHTML

è¯¦è§ `INDEX.md` äº†è§£æ•°æ®ä½¿ç”¨æ–¹æ³•ã€‚
